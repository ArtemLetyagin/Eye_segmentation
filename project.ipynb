{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292863e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numbers of bad images\n",
    "bad_ims = [6,12,25,27,29,30,32,44,49,53,56,66,67,68,76,77,78,79,88,92,93,94,98,101,105,106,109,112,113,114,122,124,136,143,144,146,150,\n",
    "151,154,157,159,160,161,163,166,168,169,171,175,179,180,219,227,229,235,252,253,255,256,257,264,265,272,275,282,283,290,294,310,\n",
    "312,318,335,341,346,355,357,358,359,360,362,369,375,381,388,390,394,396,402,407,408,411,413,414,416,420,427,429,430,431,436,437,\n",
    "438,441,442,446,447,449,453,454,455,457,458,460,461,466,467,468,476,480,485,486,487,493,496,499,502,504,505,508,509,510,512,515,\n",
    "516,518,526,528,531,532,535,537,539,541,544,546,553,555,556,559,565,567,568,569,574,578,579,586,589,591,595,598,601,603,604,607,\n",
    "619,622,625,629,640,645,648,\n",
    "1,4,7,8,9,11,13,14,16,18,19,20,21,23,26,28,31,33,34,35,36,41,47,48,51,54,57,59,61,62,63,64,69,70,71,72,73,74,80,84,85,89,90,91,\n",
    "           96,97,104,115,119,121,125,126,129,132,134,135,139,153,156,158,162,164,173,176,177,181,182,183,185,186,187,189,192,\n",
    "           194,196,198,199,204,209,213,215,222,226,231,233,234,236,237,239,240,243,244,250,251,254,261,262,266,270,276,284,287,\n",
    "           291,292,293,297,301,309,311,313,314,316,323,326,328,329,330,331,333,338,340,342,344,345,349,350,352,353,363,365,366,\n",
    "           367,368,370,371,372,374,380,384,385,399,401,404,406,410,415,417,421,424,428,434,452,463,469,475,477,482,492,494,497,\n",
    "           501,519,529,540,543,549,575,580,585,587,590,594,597,600,611,612,613,615,620,624,628,630,635,638,641,643,644,646,650]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1163e36",
   "metadata": {},
   "source": [
    "# Парсер данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, path, bad_ims):\n",
    "        img_files = sorted(glob.glob(f\"{path}/*.png\"))\n",
    "        \n",
    "        true = []\n",
    "        for i in range(len(img_files)):\n",
    "            if i not in bad_ims:\n",
    "                true.append(img_files[i])\n",
    "        self.img_files = true\n",
    "        \n",
    "        #self.transform = transform\n",
    "    def fill_polig(self, polig, image_size):\n",
    "        mask = np.zeros(image_size)\n",
    "        if len(polig) == 1:\n",
    "            cv2.fillPoly(mask, np.int32(polig), 1)\n",
    "        else:\n",
    "            for p in polig:\n",
    "                cv2.fillPoly(mask, np.int32([p]), 1)\n",
    "        return mask\n",
    "    \n",
    "    def fill_mask(self, features, image_size):\n",
    "        mask = np.zeros(image_size)\n",
    "        for feature in features:\n",
    "            polig_points = []\n",
    "            if feature['geometry']['type'] == 'MultiPolygon':\n",
    "                for polig in feature['geometry']['coordinates']:\n",
    "                    mask+=self.fill_polig(polig, image_size)\n",
    "            else:\n",
    "                points = feature['geometry']['coordinates']\n",
    "                mask+=self.fill_polig(points, image_size)\n",
    "        return mask\n",
    "\n",
    "    def cv2unsharp(self, image):\n",
    "        gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
    "        unsharp_image = cv2.addWeighted(image, 7.0, gaussian_3, -6.0, 0)\n",
    "        return unsharp_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_files[idx]\n",
    "        json_path = image_path.replace(\"png\", \"geojson\")\n",
    "        with open(json_path) as f:\n",
    "            json_contents = json.load(f)\n",
    "            \n",
    "        image = cv2.imread(self.img_files[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.cv2unsharp(image)\n",
    "        image = np.array(image / 255, dtype=np.float32)\n",
    "        image_size = image.shape[:2]\n",
    "        \n",
    "        if type(json_contents) == dict and json_contents['type'] == 'FeatureCollection':\n",
    "            features = json_contents['features']\n",
    "        elif type(json_contents) == list:\n",
    "            features = json_contents\n",
    "        else:\n",
    "            features = [json_contents]\n",
    "            \n",
    "        mask = self.fill_mask(features, image_size)\n",
    "        mask_rev = np.ones(image_size)\n",
    "        return {'image': image, 'mask': np.float32(np.stack([mask_rev-mask, mask], axis=-1))}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'path/to/data'\n",
    "dataset = EyeDataset(path, bad_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation\n",
    "transforms = A.Compose([ToTensorV2(transpose_mask=True)])\n",
    "\n",
    "aug_transform = A.Compose([\n",
    "    A.CenterCrop(900, 900, p=1),\n",
    "    A.RandomCrop(320, 320, p=1),\n",
    "    A.VerticalFlip(p=0.5),              \n",
    "    A.RandomRotate90(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb249cc1",
   "metadata": {},
   "source": [
    "# Создание даталоадера для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataseter(Dataset):\n",
    "    def __init__(self, dataset, indices, transform, aug_transform, mult):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "        self.aug_transform = aug_transform\n",
    "        self.mult = mult\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[self.indices[int(idx/self.mult)]]\n",
    "        \n",
    "        augmented = self.aug_transform(image=sample['image'], mask=sample['mask'])\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(**augmented)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)*self.mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.25)\n",
    "\n",
    "#datasets\n",
    "train_dataset = Dataseter(dataset, train_indices, transform=transforms, aug_transform=aug_transform, mult=2)\n",
    "valid_dataset = ValidDataseter(dataset, test_indices, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 1,\n",
    "                                   shuffle=True, drop_last=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, 1,\n",
    "                                   shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66352950",
   "metadata": {},
   "source": [
    "# Проверка того как работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "image = batch['image']\n",
    "mask = batch['mask']\n",
    "\n",
    "image_with_mask = draw_segmentation_masks((image[0] * 255).type(torch.uint8), (mask[0][1]).type(torch.bool))\n",
    "plt.imshow(image_with_mask.permute(2,1,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fbc19",
   "metadata": {},
   "source": [
    "# Модель и обучени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = smp.Unet('resnet50', activation='softmax', classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('path/to/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SoftDice loss\n",
    "class SoftDice():\n",
    "    def __init__(self, eps=1e-8):\n",
    "        self.eps = eps\n",
    "    def __call__(self, pred, target):\n",
    "        num = torch.sum(2*pred*target)\n",
    "        den = torch.sum(pred+target)\n",
    "        return 1 - num / (den + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08396ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = SoftDice()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        image = batch['image'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        pred = model(image)\n",
    "\n",
    "        l = loss(pred[0][1], mask[0][1])\n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(l.item())\n",
    "        \n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    print(f'epoch: {epoch}, loss: {np.sum(np.array(losses))/(len(train_loader))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fa8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataseter(Dataset):\n",
    "    def __init__(self, path, transforms):\n",
    "        self.path = path\n",
    "        self.images = os.listdir(path)\n",
    "        self.transform = transforms\n",
    "\n",
    "    def cv2unsharp(self, image):\n",
    "        gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
    "        unsharp_image = cv2.addWeighted(image, 7.0, gaussian_3, -6.0, 0)\n",
    "        return unsharp_image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.path+f'/{self.images[idx]}')\n",
    "        image = self.cv2unsharp(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        image = np.array(image / 255, dtype=np.float32)\n",
    "        sample = {'image': image, 'mask': np.zeros((10,10))}\n",
    "        \n",
    "        return {'image': self.transform(**sample)['image'], 'idx': self.images[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "def sliding_window(x, window_size=320, stride=50):\n",
    "    h = x.shape[2]\n",
    "    w = x.shape[3]\n",
    "    final = torch.zeros((2,h,w)).to(device)\n",
    "    for i in range(0, x.shape[2]-window_size, stride):\n",
    "        for j in range(0, x.shape[3]-window_size, stride):\n",
    "            crop = x[:,:,i:i+window_size, j:j+window_size]\n",
    "            mask = model(crop)\n",
    "            zero = torch.zeros((window_size,window_size)).to(device)\n",
    "            one = torch.ones((window_size,window_size)).to(device)\n",
    "\n",
    "            pred_mask2 = torch.where(mask[0][0]<mask[0][1], one, zero)\n",
    "            pred_mask1 = one-pred_mask2\n",
    "            pred_mask = torch.stack((pred_mask1, pred_mask2)).to(device)\n",
    "            \n",
    "            final[:, i:i+window_size, j:j+window_size]+=pred_mask\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataseter('path/to/test_dataset', transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75bdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(test_loader):\n",
    "    image = batch['image']\n",
    "    idx = batch['idx'][0]\n",
    "    a = sliding_window(image.to(device))\n",
    "\n",
    "    s = a.shape[1:3]\n",
    "    zero = torch.zeros(s).to(device)\n",
    "    one = torch.ones(s).to(device)\n",
    "    mean = torch.sum(a[1])/(a[1].shape[0]*a[1].shape[1]-torch.sum(a[1]==0))\n",
    "    pred_mask2 = torch.where(a[1]>(mean), one, zero)\n",
    "    pred_mask1 = one - pred_mask2\n",
    "    pred_mask = torch.stack((pred_mask1, pred_mask2))\n",
    "\n",
    "    im = (((torch.stack((pred_mask[1], pred_mask[1], pred_mask[1]), 0)*255).permute(1,2,0)).cpu()).numpy()\n",
    "    cv2.imwrite(f'path/to/save/images/{idx}', im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
